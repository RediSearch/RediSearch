Excellent! I can see the logs now. Let me search for the critical parts - specifically when WORKERS was changed to 0 and what happened after. Let me look for connection-related issues:

Perfect! Now I can see the **exact problem**! This is a **DEADLOCK**, not just connection starvation. Let me analyze what happened:

## **üî¥ CRITICAL FINDING: DEADLOCK DURING WORKERS TERMINATION**

Looking at the logs, here's what happened:

### **Timeline of the Hang:**

1. **15:06:27.199** - First WORKERS‚Üí0 change:
   - Successfully disabled workers
   - Freed 24 connections (switching to "Freeing" state)
   - Created new thread pool of size 1

2. **15:06:30.200** - Second WORKERS‚Üí0 change (test toggling):
   - Started terminating worker threads
   - **CRITICAL**: Connections are being freed **WHILE** worker threads are still executing queries
   - Lines 3200-3207 show clients being closed with active commands (`_FT.AGGREGATE`, `_FT.CURSOR`)

3. **Stack trace at line 6743-6862** shows **DEADLOCK**:
   - Main thread is stuck in `workersThreadPool_SetNumWorkers` ‚Üí `barrier_wait_and_destroy`
   - Worker thread `workers-0902` is stuck at `pthread_barrier_wait`
   - Worker thread `workers-0558` is stuck trying to acquire `pthread_mutex_lock` ‚Üí `RM_ThreadSafeContextLock`

### **The Root Cause:**

This is **NOT** about connection starvation with 1 connection. This is a **RACE CONDITION** in the code:

```
Thread 1 (Main):                    Thread 2 (Worker):
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CONFIG SET WORKERS 0
  ‚Üì
Terminate worker threads
  ‚Üì
Wait at barrier for workers          Executing query (_FT.AGGREGATE)
to finish                              ‚Üì
  ‚Üì                                  Try to acquire ThreadSafeContext lock
  ‚Üì                                    ‚Üì
  ‚Üì                                  BLOCKED (lock held by main thread?)
  ‚Üì                                    ‚Üì
DEADLOCK ‚ùå                          DEADLOCK ‚ùå
```

### **Why Setting SEARCH_THREADS=1 Triggers It:**

With `SEARCH_THREADS=1`, there's **only 1 search thread** available to process queries. When you:
1. Have queries running
2. Change WORKERS to 0
3. The code tries to terminate worker threads
4. But those threads are waiting for the search thread to complete their work
5. **DEADLOCK**: Main thread waits for workers, workers wait for search thread, search thread might be blocked

---

## **Analysis:**

**This is NOT the MOD-11658 connection starvation bug!** This is a **DIFFERENT BUG** - a **deadlock during worker thread termination when SEARCH_THREADS=1**.

### **The Connection Starvation Bug (MOD-11658):**
- Happens when WORKERS=0 and queries try to use connections
- With 1 connection per shard, concurrent queries compete
- **Symptom**: Timeouts, slow queries, unresponsiveness
- **Your fix**: Minimum 2 connections prevents this

### **The Deadlock Bug (What you're seeing now):**
- Happens when changing WORKERS while queries are running
- With SEARCH_THREADS=1, there's resource contention
- **Symptom**: Complete hang, no progress
- **Root cause**: Race condition in worker thread termination logic

---

## **Recommendations:**

### **1. This is a SEPARATE bug from MOD-11658**
The connection starvation fix is still valid and important. This deadlock is a different issue.

### **2. The deadlock happens because:**
- Worker threads are terminated while holding resources
- With SEARCH_THREADS=1, there's insufficient parallelism to drain pending work
- The barrier wait in `workersThreadPool_SetNumWorkers` deadlocks

### **3. To test MOD-11658 properly:**
- Use `SEARCH_THREADS` > 1 (default or higher)
- Or don't toggle WORKERS rapidly while queries are running
- The connection starvation issue is about **steady-state** behavior with WORKERS=0, not the transition

### **4. To fix the deadlock bug:**
- Need to drain pending work before terminating workers
- Or use a timeout on the barrier wait
- Or allow workers to complete their current task before terminating

---

## **Summary:**

‚úÖ **You found a REAL bug** - but it's a different one!  
‚úÖ **MOD-11658 (connection starvation)** - Your fix is correct  
‚ùå **NEW BUG (deadlock on WORKERS change)** - Needs separate fix  

The hang you're seeing is **NOT** because of 1 connection. It's because of a **deadlock in the worker thread termination code** when `SEARCH_THREADS=1` and queries are active.

**To test MOD-11658 properly**: Use `SEARCH_THREADS` > 1 or don't change WORKERS while queries are running.


Now let me search for when WORKERS was actually changed to 0:

**EXCELLENT!** Now I can see the **REAL problem**! This is a **SEGMENTATION FAULT (SIGSEGV)** during the `FT.CONFIG SET WORKERS 0` command, NOT a deadlock!

Let me analyze the crash:

Perfect! Now I have the complete picture. Let me provide a comprehensive analysis:

---

## **üî¥ CRITICAL FINDING: SEGMENTATION FAULT (SIGSEGV) During WORKERS Change**

### **What Actually Happened:**

The test **did NOT hang** - it **CRASHED** with a segmentation fault (signal 11)!

**Timeline:**
1. **17:52:31.073** - First `FT.CONFIG SET WORKERS 0` - ‚úÖ **SUCCESS**
2. **17:52:33.084** - `FT.CONFIG SET WORKERS 8` - ‚úÖ **SUCCESS** (created 8 worker threads)
3. **17:52:34.139** - Second `FT.CONFIG SET WORKERS 0` - ‚ùå **CRASH**
   - Redis crashed with SIGSEGV (signal 11)
   - Crashed at address: `0x3e80034afb5` (invalid memory access)
   - Crashed in `clock_nanosleep` (inside `barrier_wait_and_destroy`)

**The crash happened while:**
- **Current command**: `_FT.CONFIG SET WORKERS 0` (client id=32)
- **Active queries**: 1143 `_FT.CURSOR` calls (ALL FAILED), 1125 `_FT.AGGREGATE` calls (ALL FAILED)
- **Worker threads**: Stuck at `pthread_barrier_wait` waiting to be terminated
- **Main thread**: Stuck in `barrier_wait_and_destroy` ‚Üí `usleep` ‚Üí `clock_nanosleep`

---

## **Root Cause Analysis:**

### **This is a RACE CONDITION in the worker thread termination code**

Looking at the stack trace (lines 6022-6033):

```
barrier_wait_and_destroy
  at /home/dor-forer/repos/RediSearch/deps/thpool/barrier.c:28:7
redisearch_thpool_broadcast_new_state
  at /home/dor-forer/repos/RediSearch/deps/thpool/thpool.c:1025:3
redisearch_thpool_terminate_when_empty
  at /home/dor-forer/repos/RediSearch/deps/thpool/thpool.c:527:3
workersThreadPool_SetNumWorkers
  at /home/dor-forer/repos/RediSearch/src/util/workers.c:88:5
setWorkThreads
  at /home/dor-forer/repos/RediSearch/src/config.c:385:3
```

**The problem:**
1. When changing WORKERS from 8 ‚Üí 0 **while queries are running**
2. The code tries to terminate worker threads using a barrier
3. Worker threads are **stuck** waiting for work to complete
4. The barrier wait times out or corrupts memory
5. **CRASH** when accessing invalid memory

---

## **Why This is NOT MOD-11658:**

| Issue | MOD-11658 (Connection Starvation) | Current Bug (Crash on WORKERS Change) |
|-------|-----------------------------------|----------------------------------------|
| **Trigger** | WORKERS=0 in steady state | Changing WORKERS while queries run |
| **Symptom** | Timeouts, slow queries | **SEGFAULT / CRASH** |
| **Root Cause** | 1 connection per shard insufficient | Race condition in thread termination |
| **When** | After WORKERS=0 is set | **During** WORKERS change |
| **Fix** | Minimum 2 connections | Need to drain work before terminating threads |

---

## **Why Your Test Keeps Crashing:**

Your test is **too aggressive** - it's changing WORKERS rapidly while queries are running:

```python
# Test toggles WORKERS every 2 seconds
for i in range(5):
    env.cmd(config_cmd(), 'SET', 'WORKERS', '0')  # ‚Üê CRASH HERE
    time.sleep(2)
    env.cmd(config_cmd(), 'SET', 'WORKERS', '8')
    time.sleep(2)
```

With **20 concurrent query threads** hammering the system, when you change WORKERS:
1. Worker threads have pending work
2. Barrier wait can't complete because threads are busy
3. Memory corruption or invalid access
4. **CRASH**

---

## **The Real Issues:**

### **Issue #1: MOD-11658 (Connection Starvation) - VALID**
- **When**: WORKERS=0 in steady state
- **Fix**: Minimum 2 connections per IO thread ‚úÖ **Your fix is correct**
- **Test**: Need to test steady-state behavior, not rapid toggling

### **Issue #2: CRASH on WORKERS Change (NEW BUG) - CRITICAL**
- **When**: Changing WORKERS while queries are running
- **Fix**: Need to properly drain pending work before terminating threads
- **Test**: Your current test is actually exposing this bug!

---

## **Recommendations:**

### **1. To Test MOD-11658 (Connection Starvation):**

**Modify your test to NOT toggle WORKERS rapidly:**

```python
def test_workers_zero_connection_starvation(env):
    env = Env(moduleArgs='WORKERS 8 SEARCH_IO_THREADS 20')
    
    # Create index and add documents
    # ...
    
    # Set WORKERS to 0 ONCE
    env.cmd(config_cmd(), 'SET', 'WORKERS', '0')
    
    # NOW run queries for extended period
    # This tests steady-state behavior with WORKERS=0
    errors = []
    for i in range(100):  # Run many queries
        try:
            result = env.cmd('FT.SEARCH', 'idx', '*', 'LIMIT', '0', '100')
            # Also test aggregations
            result = env.cmd('FT.AGGREGATE', 'idx', '*', 'GROUPBY', '1', '@field1', 'REDUCE', 'COUNT', '0')
        except Exception as e:
            errors.append(str(e))
    
    # Check for timeouts/errors
    assert len(errors) == 0, f"Connection starvation detected: {errors}"
```

### **2. To Test/Fix the CRASH Bug:**

This is a **separate bug** that needs investigation:
- The thread pool termination logic has a race condition
- Need to ensure all pending work is drained before terminating threads
- May need timeout handling or better synchronization

### **3. Immediate Action:**

**Your connection pool fix is still valid and important!** But you need to:
1. Test it with steady-state WORKERS=0 (not rapid toggling)
2. File a **separate bug** for the crash on WORKERS change
3. Consider adding a warning: "Do not change WORKERS while queries are running"

---

## **Summary:**

‚úÖ **MOD-11658 (Connection Starvation)** - Your fix is correct, but test needs adjustment  
‚ùå **NEW BUG (CRASH on WORKERS change)** - Critical race condition in thread termination  
üéØ **Your test is valuable** - It exposed a different critical bug!  

The "hang" you're seeing is actually a **CRASH** that happens when changing WORKERS while queries are running. This is **NOT** related to connection pool size - it's a race condition in the worker thread termination code.
